{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31239,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**IMPORTS AND SETUP**","metadata":{}},{"cell_type":"code","source":"\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nimport xgboost as xgb\nimport joblib\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set random seed for reproducibility\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)\n\n# Plotting configuration\nplt.style.use('seaborn-v0_8-darkgrid')\nsns.set_palette(\"husl\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-11T21:45:45.174730Z","iopub.execute_input":"2026-01-11T21:45:45.175055Z","iopub.status.idle":"2026-01-11T21:45:49.956562Z","shell.execute_reply.started":"2026-01-11T21:45:45.175020Z","shell.execute_reply":"2026-01-11T21:45:49.955530Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"**DATA LOADING**","metadata":{}},{"cell_type":"code","source":"\n# Load the dataset\ndf = pd.read_csv('/kaggle/input/vehicle-sales-data/car_prices.csv')\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"DATASET OVERVIEW\")\nprint(\"=\"*70)\nprint(f\"Dataset shape: {df.shape}\")\nprint(f\"\\nFirst few rows:\")\nprint(df.head())\nprint(f\"\\nData types:\")\nprint(df.dtypes)\nprint(f\"\\nMissing values:\")\nprint(df.isnull().sum())\nprint(f\"\\nBasic statistics:\")\nprint(df.describe())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-11T21:45:52.146223Z","iopub.execute_input":"2026-01-11T21:45:52.147281Z","iopub.status.idle":"2026-01-11T21:45:52.308335Z","shell.execute_reply.started":"2026-01-11T21:45:52.147233Z","shell.execute_reply":"2026-01-11T21:45:52.306759Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/3389352196.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/kaggle/input/vehicle-sales-data/car_prices.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"=\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m70\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DATASET OVERVIEW\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/vehicle-sales-data/car_prices.csv'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/input/vehicle-sales-data/car_prices.csv'","output_type":"error"}],"execution_count":2},{"cell_type":"markdown","source":"**OUTLIER DETECTION AND FILTERING**","metadata":{}},{"cell_type":"code","source":"\nprint(\"\\n\" + \"=\"*70)\nprint(\"OUTLIER FILTERING\")\nprint(\"=\"*70)\n\n# Store original size\noriginal_size = len(df)\n\n# Filter outliers based on domain knowledge\ndf_filtered = df[\n    (df['year'] >= 1990) &\n    (df['year'] <= 2026) &\n    (df['odometer'] > 0) &\n    (df['odometer'] <= 500000) &\n    (df['sellingprice'] > 500) &\n    (df['sellingprice'] <= 150000) &\n    (df['condition'] >= 1) &\n    (df['condition'] <= 49)\n].copy()\n\n# Additional statistical outlier removal using IQR for sellingprice\nQ1 = df_filtered['sellingprice'].quantile(0.01)\nQ3 = df_filtered['sellingprice'].quantile(0.99)\nIQR = Q3 - Q1\nlower_bound = Q1 - 1.5 * IQR\nupper_bound = Q3 + 1.5 * IQR\n\ndf_filtered = df_filtered[\n    (df_filtered['sellingprice'] >= lower_bound) &\n    (df_filtered['sellingprice'] <= upper_bound)\n]\n\nprint(f\"Original dataset: {original_size:,} rows\")\nprint(f\"After filtering: {len(df_filtered):,} rows\")\nprint(f\"Removed: {original_size - len(df_filtered):,} rows ({100*(original_size - len(df_filtered))/original_size:.2f}%)\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**STRATIFIED SAMPLING**","metadata":{}},{"cell_type":"code","source":"\nprint(\"\\n\" + \"=\"*70)\nprint(\"STRATIFIED SAMPLING\")\nprint(\"=\"*70)\n\n# Target sample size (10-20% of filtered data)\ntarget_size = min(100000, int(len(df_filtered) * 0.2))\n\n# Create stratification bins\ndf_filtered['year_bin'] = pd.cut(df_filtered['year'], bins=10, labels=False)\ndf_filtered['price_bin'] = pd.qcut(df_filtered['sellingprice'], q=10, labels=False, duplicates='drop')\n\n# Perform stratified sampling\ndf_sample = df_filtered.groupby(['year_bin', 'price_bin'], group_keys=False).apply(\n    lambda x: x.sample(min(len(x), max(1, int(len(x) * target_size / len(df_filtered)))), random_state=RANDOM_STATE)\n).reset_index(drop=True)\n\n# Remove temporary binning columns\ndf_sample = df_sample.drop(['year_bin', 'price_bin'], axis=1)\n\nprint(f\"Target sample size: {target_size:,}\")\nprint(f\"Actual sample size: {len(df_sample):,}\")\nprint(f\"Sampling ratio: {100*len(df_sample)/len(df_filtered):.2f}%\")\n\n# Verify distribution preservation\nprint(\"\\nYear distribution comparison:\")\nprint(\"Original:\")\nprint(df_filtered['year'].value_counts(bins=5, sort=False).head())\nprint(\"\\nSampled:\")\nprint(df_sample['year'].value_counts(bins=5, sort=False).head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**ATA CLEANING AND STANDARDIZATION**","metadata":{}},{"cell_type":"code","source":"\nprint(\"\\n\" + \"=\"*70)\nprint(\"DATA CLEANING AND STANDARDIZATION\")\nprint(\"=\"*70)\n\n# 1. Handle missing values first\ndf_sample['make'] = df_sample['make'].fillna('Unknown')\ndf_sample['model'] = df_sample['model'].fillna('Unknown')\ndf_sample['body'] = df_sample['body'].fillna('Unknown')\ndf_sample['transmission'] = df_sample['transmission'].fillna('unknown')\n\n# 2. Standardize make (brand) - convert to title case\ndf_sample['make'] = df_sample['make'].astype(str).str.strip().str.title()\nprint(f\"Unique makes after cleaning: {df_sample['make'].nunique()}\")\n\n# 3. Standardize model - convert to title case and strip whitespace\ndf_sample['model'] = df_sample['model'].astype(str).str.strip().str.title()\nprint(f\"Unique models after cleaning: {df_sample['model'].nunique()}\")\n\n# 4. Standardize and classify body types\ndef standardize_body_type(body):\n    \"\"\"Standardize body type classifications\"\"\"\n    if pd.isna(body):\n        return 'Other'\n    \n    body = str(body).lower().strip()\n    \n    # Coupe variations\n    if 'coupe' in body or 'cpe' in body:\n        return 'Coupe'\n    \n    # Sedan variations\n    if 'sedan' in body or 'sdn' in body:\n        return 'Sedan'\n    \n    # SUV variations\n    if 'suv' in body or 'sport utility' in body:\n        return 'SUV'\n    \n    # Truck variations\n    if 'truck' in body or 'pickup' in body:\n        return 'Truck'\n    \n    # Van variations\n    if 'van' in body or 'minivan' in body:\n        return 'Van'\n    \n    # Wagon variations\n    if 'wagon' in body or 'wgn' in body:\n        return 'Wagon'\n    \n    # Convertible variations\n    if 'convertible' in body or 'conv' in body or 'cabriolet' in body:\n        return 'Convertible'\n    \n    # Hatchback variations\n    if 'hatchback' in body or 'hatch' in body:\n        return 'Hatchback'\n    \n    # Default\n    return 'Other'\n\ndf_sample['body'] = df_sample['body'].apply(standardize_body_type)\nprint(f\"\\nBody types after standardization:\")\nprint(df_sample['body'].value_counts())\n\n# 5. Standardize transmission\ndf_sample['transmission'] = df_sample['transmission'].astype(str).str.strip().str.lower()\n\nprint(f\"\\nMissing values after cleaning:\")\nprint(df_sample.isnull().sum())\n\nprint(f\"\\nData cleaning summary:\")\nprint(f\"  - Makes standardized: {df_sample['make'].nunique()} unique brands\")\nprint(f\"  - Models standardized: {df_sample['model'].nunique()} unique models\")\nprint(f\"  - Body types reduced to standard categories\")\nprint(f\"  - Transmission values normalized\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**ENCODING CATEGORICAL VARIABLES**","metadata":{}},{"cell_type":"code","source":"\nprint(\"\\n\" + \"=\"*70)\nprint(\"ENCODING CATEGORICAL VARIABLES\")\nprint(\"=\"*70)\n\n# Create label encoders for categorical variables\ncategorical_cols = ['make', 'model', 'body', 'transmission']\nlabel_encoders = {}\n\nfor col in categorical_cols:\n    le = LabelEncoder()\n    df_sample[f'{col}_encoded'] = le.fit_transform(df_sample[col].astype(str))\n    label_encoders[col] = le\n    print(f\"{col}: {len(le.classes_)} unique values\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**PREPARE FEATURES AND TARGET**","metadata":{}},{"cell_type":"code","source":"\nprint(\"\\n\" + \"=\"*70)\nprint(\"PREPARING FEATURES AND TARGET\")\nprint(\"=\"*70)\n\n# Select features for the model (excluding mmr)\nfeature_cols = [\n    'year', 'condition', 'odometer',\n    'make_encoded', 'model_encoded', 'body_encoded', 'transmission_encoded'\n]\n\nX = df_sample[feature_cols]\ny = df_sample['sellingprice']\n\nprint(f\"Features shape: {X.shape}\")\nprint(f\"Target shape: {y.shape}\")\nprint(f\"\\nFeatures used:\")\nfor i, col in enumerate(feature_cols, 1):\n    print(f\"  {i}. {col}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**TRAIN-TEST SPLIT**","metadata":{}},{"cell_type":"code","source":"\nprint(\"\\n\" + \"=\"*70)\nprint(\"TRAIN-TEST SPLIT\")\nprint(\"=\"*70)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=RANDOM_STATE\n)\n\nprint(f\"Training set: {X_train.shape[0]:,} samples\")\nprint(f\"Test set: {X_test.shape[0]:,} samples\")\nprint(f\"Split ratio: 80/20\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**MODEL TRAINING - XGBOOST**","metadata":{}},{"cell_type":"code","source":"\nprint(\"\\n\" + \"=\"*70)\nprint(\"XGBOOST MODEL TRAINING\")\nprint(\"=\"*70)\n\n# Define XGBoost parameters\nparams = {\n    'objective': 'reg:squarederror',\n    'max_depth': 6,\n    'learning_rate': 0.1,\n    'n_estimators': 200,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'random_state': RANDOM_STATE,\n    'n_jobs': -1,\n    'eval_metric': 'rmse'\n}\n\nprint(\"Model parameters:\")\nfor key, value in params.items():\n    print(f\"  {key}: {value}\")\n\n# Train the model\nprint(\"\\nTraining XGBoost model...\")\ntry:\n    model = xgb.XGBRegressor(**params)\n    model.fit(X_train, y_train, verbose=False)\n    print(\"Training completed successfully!\")\nexcept Exception as e:\n    print(f\"Error during training: {e}\")\n    raise","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**MODEL EVALUATION**","metadata":{}},{"cell_type":"code","source":"\nprint(\"\\n\" + \"=\"*70)\nprint(\"MODEL EVALUATION\")\nprint(\"=\"*70)\n\n# Verify model was trained\nif 'model' not in locals():\n    raise NameError(\"Model not found!\")\n\n# Make predictions\ny_train_pred = model.predict(X_train)\ny_test_pred = model.predict(X_test)\n\n# Calculate metrics\ntrain_r2 = r2_score(y_train, y_train_pred)\ntest_r2 = r2_score(y_test, y_test_pred)\ntrain_mae = mean_absolute_error(y_train, y_train_pred)\ntest_mae = mean_absolute_error(y_test, y_test_pred)\ntrain_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\ntest_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\ntrain_mape = np.mean(np.abs((y_train - y_train_pred) / y_train)) * 100\ntest_mape = np.mean(np.abs((y_test - y_test_pred) / y_test)) * 100\n\nprint(\"\\nPERFORMANCE METRICS:\")\nprint(\"-\" * 50)\nprint(f\"{'Metric':<20} {'Training':<15} {'Test':<15}\")\nprint(\"-\" * 50)\nprint(f\"{'R² Score':<20} {train_r2:<15.4f} {test_r2:<15.4f}\")\nprint(f\"{'MAE ($)':<20} {train_mae:<15.2f} {test_mae:<15.2f}\")\nprint(f\"{'RMSE ($)':<20} {train_rmse:<15.2f} {test_rmse:<15.2f}\")\nprint(f\"{'MAPE (%)':<20} {train_mape:<15.2f} {test_mape:<15.2f}\")\nprint(\"-\" * 50)\n\n# Cross-validation\nprint(\"\\nPerforming 5-fold cross-validation...\")\ncv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')\nprint(f\"CV R² scores: {cv_scores}\")\nprint(f\"Mean CV R²: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**FEATURE IMPORTANCE ANALYSIS**","metadata":{}},{"cell_type":"code","source":"\nprint(\"\\n\" + \"=\"*70)\nprint(\"FEATURE IMPORTANCE ANALYSIS\")\nprint(\"=\"*70)\n\n# Get feature importance\nimportance_df = pd.DataFrame({\n    'feature': feature_cols,\n    'importance': model.feature_importances_\n}).sort_values('importance', ascending=False)\n\nprint(\"\\nFeature Importance Ranking:\")\nprint(\"-\" * 40)\nfor idx, row in importance_df.iterrows():\n    print(f\"{row['feature']:<25} {row['importance']:.4f}\")\n\n# Visualize feature importance\nplt.figure(figsize=(10, 6))\nplt.barh(importance_df['feature'], importance_df['importance'])\nplt.xlabel('Importance Score')\nplt.title('Feature Importances - XGBoost Model')\nplt.gca().invert_yaxis()\nplt.tight_layout()\nplt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**SAVE MODEL AND ENCODERS**","metadata":{}},{"cell_type":"code","source":"\nprint(\"\\n\" + \"=\"*70)\nprint(\"SAVING MODEL AND ENCODERS\")\nprint(\"=\"*70)\n\n# Save the model\njoblib.dump(model, 'vehicle_price_model.pkl')\nprint(\"Model saved as 'vehicle_price_model.pkl'\")\n\n# Save label encoders\njoblib.dump(label_encoders, 'label_encoders.pkl')\nprint(\"Label encoders saved as 'label_encoders.pkl'\")\n\n# Save feature columns\njoblib.dump(feature_cols, 'feature_columns.pkl')\nprint(\"Feature columns saved as 'feature_columns.pkl'\")\n\n# Save make-model mapping for UI dropdown\n# Handle any NaN values in model column before creating mapping\nmake_model_mapping = {}\nfor make in df_sample['make'].unique():\n    models = df_sample[df_sample['make'] == make]['model'].dropna().unique()\n    # Convert to string and sort\n    models = sorted([str(m) for m in models])\n    if models:  # Only add if there are models\n        make_model_mapping[make] = models\n\njoblib.dump(make_model_mapping, 'make_model_mapping.pkl')\nprint(\"Make-model mapping saved as 'make_model_mapping.pkl'\")\n\n# Save sample statistics for later use (updated with hybrid metrics)\nstatistics = {\n    'train_size': len(X_train),\n    'test_size': len(X_test),\n    # ML-only metrics\n    'ml_r2_score': test_r2,\n    'ml_mae': test_mae,\n    'ml_rmse': test_rmse,\n    'ml_mape': test_mape,\n}\njoblib.dump(statistics, 'model_statistics.pkl')\nprint(\"Model statistics saved as 'model_statistics.pkl'\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**RULE-BASED SYSTEM INTEGRATION**","metadata":{}},{"cell_type":"code","source":"\nprint(\"\\n\" + \"=\"*70)\nprint(\"RULE-BASED SYSTEM INTEGRATION\")\nprint(\"=\"*70)\n\n# Define the rule-based system class\nclass VehiclePricingRules:\n    \"\"\"Expert rule-based system for vehicle pricing\"\"\"\n    \n    def __init__(self):\n        self.rules_applied = []\n        self.warnings = []\n    \n    def reset(self):\n        self.rules_applied = []\n        self.warnings = []\n    \n    def adjust_prediction(self, base_prediction, vehicle_data):\n        \"\"\"Apply expert rules to adjust ML model prediction\"\"\"\n        adjusted_price = base_prediction\n        adjustments = []\n        \n        year = vehicle_data.get('year')\n        odometer = vehicle_data.get('odometer')\n        condition = vehicle_data.get('condition')\n        make = vehicle_data.get('make', '')\n        model = vehicle_data.get('model', '')\n        body = vehicle_data.get('body', '')\n        transmission = vehicle_data.get('transmission', '')\n        \n        # Rule 1: Luxury Depreciation Curve\n        luxury_brands = ['Bmw', 'Mercedes-Benz', 'Audi', 'Lexus', 'Porsche', 'Tesla']\n        if make in luxury_brands and year:\n            vehicle_age = 2026 - year\n            if vehicle_age > 9:\n                depreciation_factor = 1 - (vehicle_age - 9) * 0.01\n                adjustment = adjusted_price * (depreciation_factor - 1)\n                adjusted_price *= depreciation_factor\n                adjustments.append(('Luxury depreciation', adjustment))\n        \n        # Rule 2: Popular Truck Premium\n        popular_trucks = ['F-150', 'Silverado', 'Ram', 'Tundra', 'Tacoma']\n        if body == 'Truck' and any(truck in model for truck in popular_trucks):\n            premium = base_prediction * 0.01\n            adjusted_price += premium\n            adjustments.append(('Popular truck premium', premium))\n        \n        # Rule 3: High Mileage Penalty\n        if odometer and odometer > 190000:\n            penalty_rate = min((odometer - 190000) / 100000 * 0.029, 0.1)\n            penalty = base_prediction * penalty_rate\n            adjusted_price -= penalty\n            adjustments.append(('High mileage penalty', -penalty))\n        \n        # Rule 4: Excellent Condition Bonus\n        if condition and condition >= 47:\n            bonus = base_prediction * 0.035\n            adjusted_price += bonus\n            adjustments.append(('Excellent condition bonus', bonus))\n        \n        # Rule 5: Poor Condition Penalty\n        elif condition and condition <= 10:\n            penalty = base_prediction * 0.08\n            adjusted_price -= penalty\n            adjustments.append(('Poor condition penalty', -penalty))\n        \n        # Rule 6: Low Mileage Premium\n        if odometer and year:\n            vehicle_age = 2026 - year\n            expected_mileage = vehicle_age * 14000\n            if odometer < expected_mileage * 0.15 and vehicle_age > 2:\n                premium = base_prediction * 0.028\n                adjusted_price += premium\n                adjustments.append(('Low mileage premium', premium))\n        \n        # Ensure price doesn't go below minimum\n        adjusted_price = max(adjusted_price, 180)\n        \n        return {\n            'adjusted_price': adjusted_price,\n            'base_price': base_prediction,\n            'adjustments': adjustments,\n            'total_adjustment': adjusted_price - base_prediction\n        }\n\n# Initialize rule-based system\nrules_engine = VehiclePricingRules()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**HYBRID SYSTEM EVALUATION**","metadata":{}},{"cell_type":"code","source":"\nprint(\"\\n\" + \"=\"*70)\nprint(\"HYBRID SYSTEM EVALUATION (ML + Rules)\")\nprint(\"=\"*70)\n\n# Check if we have test predictions\nif 'y_test_pred' not in locals():\n    print(\"ERROR: Test predictions not found. Skipping hybrid evaluation.\")\n    print(\"Please ensure Part 10 (Model Evaluation) ran successfully.\")\n    # Set default values\n    hybrid_r2 = test_r2\n    hybrid_mae = test_mae\n    hybrid_rmse = test_rmse\n    hybrid_mape = test_mape\n    rule_adjustments_list = np.zeros(len(y_test))\nelse:\n    # Apply rules to test set predictions\n    print(\"\\nApplying rule-based adjustments to test set...\")\n    hybrid_predictions = []\n    rule_adjustments_list = []\n\n    for idx in range(len(y_test)):\n        # Get the ML prediction\n        ml_pred = y_test_pred[idx]\n        \n        # Get vehicle data\n        test_idx = X_test.index[idx]\n        vehicle_data = {\n            'year': X_test.iloc[idx]['year'],\n            'make': df_sample.loc[test_idx, 'make'],\n            'model': df_sample.loc[test_idx, 'model'],\n            'body': df_sample.loc[test_idx, 'body'],\n            'transmission': df_sample.loc[test_idx, 'transmission'],\n            'condition': X_test.iloc[idx]['condition'],\n            'odometer': X_test.iloc[idx]['odometer']\n        }\n        \n        # Apply rules\n        result = rules_engine.adjust_prediction(ml_pred, vehicle_data)\n        hybrid_predictions.append(result['adjusted_price'])\n        rule_adjustments_list.append(result['total_adjustment'])\n\n    hybrid_predictions = np.array(hybrid_predictions)\n    rule_adjustments_list = np.array(rule_adjustments_list)\n\n    # Calculate hybrid system metrics\n    hybrid_r2 = r2_score(y_test, hybrid_predictions)\n    hybrid_mae = mean_absolute_error(y_test, hybrid_predictions)\n    hybrid_rmse = np.sqrt(mean_squared_error(y_test, hybrid_predictions))\n    hybrid_mape = np.mean(np.abs((y_test - hybrid_predictions) / y_test)) * 100\n\n    print(\"\\nHYBRID SYSTEM PERFORMANCE:\")\n    print(\"=\" * 70)\n    print(f\"{'Metric':<20} {'ML Only':<15} {'Hybrid (ML+Rules)':<20} {'Improvement':<15}\")\n    print(\"-\" * 70)\n    print(f\"{'R² Score':<20} {test_r2:<15.4f} {hybrid_r2:<20.4f} {(hybrid_r2-test_r2)*100:+.2f}%\")\n    print(f\"{'MAE ($)':<20} {test_mae:<15.2f} {hybrid_mae:<20.2f} {test_mae-hybrid_mae:+.2f}\")\n    print(f\"{'RMSE ($)':<20} {test_rmse:<15.2f} {hybrid_rmse:<20.2f} {test_rmse-hybrid_rmse:+.2f}\")\n    print(f\"{'MAPE (%)':<20} {test_mape:<15.2f} {hybrid_mape:<20.2f} {test_mape-hybrid_mape:+.2f}%\")\n    print(\"=\" * 70)\n\n    # Statistics on rule adjustments\n    print(\"\\nRULE ADJUSTMENT STATISTICS:\")\n    print(\"-\" * 50)\n    print(f\"Average adjustment: ${np.mean(rule_adjustments_list):+,.2f}\")\n    print(f\"Median adjustment: ${np.median(rule_adjustments_list):+,.2f}\")\n    print(f\"Max positive adjustment: ${np.max(rule_adjustments_list):+,.2f}\")\n    print(f\"Max negative adjustment: ${np.min(rule_adjustments_list):+,.2f}\")\n    print(f\"% predictions adjusted: {np.sum(rule_adjustments_list != 0) / len(rule_adjustments_list) * 100:.1f}%\")\n\n    # Visualize hybrid system improvements\n    try:\n        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n\n        # Plot 1: ML vs Hybrid comparison\n        axes[0].scatter(y_test, y_test_pred, alpha=0.3, s=10, label='ML Only', color='blue')\n        axes[0].scatter(y_test, hybrid_predictions, alpha=0.3, s=10, label='Hybrid', color='green')\n        axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n        axes[0].set_xlabel('Actual Price ($)')\n        axes[0].set_ylabel('Predicted Price ($)')\n        axes[0].set_title(f'ML vs Hybrid Predictions\\nHybrid R²={hybrid_r2:.4f} vs ML R²={test_r2:.4f}')\n        axes[0].legend()\n        axes[0].grid(True, alpha=0.3)\n\n        # Plot 2: Rule adjustments distribution\n        axes[1].hist(rule_adjustments_list, bins=50, edgecolor='black', alpha=0.7)\n        axes[1].axvline(x=0, color='r', linestyle='--', lw=2, label='No adjustment')\n        axes[1].set_xlabel('Adjustment Amount ($)')\n        axes[1].set_ylabel('Frequency')\n        axes[1].set_title('Distribution of Rule-Based Adjustments')\n        axes[1].legend()\n        axes[1].grid(True, alpha=0.3)\n\n        plt.tight_layout()\n        plt.savefig('hybrid_system_analysis.png', dpi=300, bbox_inches='tight')\n        print(\"\\nHybrid system analysis plot saved as 'hybrid_system_analysis.png'\")\n        plt.show()\n    except Exception as e:\n        print(f\"\\nWarning: Could not generate hybrid system plots: {e}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**EXAMPLE PREDICTIONS WITH HYBRID SYSTEM**","metadata":{}},{"cell_type":"code","source":"\nprint(\"\\n\" + \"=\"*70)\nprint(\"EXAMPLE PREDICTIONS - HYBRID SYSTEM\")\nprint(\"=\"*70)\n\n# Create example predictions\nexamples = [\n    {'year': 2015, 'make': 'Ford', 'model': 'F-150', 'body': 'Truck', \n     'transmission': 'automatic', 'condition': 35, 'odometer': 50000},\n    {'year': 2020, 'make': 'Toyota', 'model': 'Camry', 'body': 'Sedan',\n     'transmission': 'automatic', 'condition': 40, 'odometer': 25000},\n    {'year': 2015, 'make': 'Bmw', 'model': '3 Series', 'body': 'Sedan',\n     'transmission': 'automatic', 'condition': 35, 'odometer': 85000}\n]\n\nprint(\"\\nComparing ML-only vs Hybrid predictions:\")\nprint(\"=\" * 90)\n\nfor i, ex in enumerate(examples, 1):\n    # Prepare features\n    ex_df = pd.DataFrame([ex])\n    \n    # Encode categoricals\n    for col in categorical_cols:\n        if ex[col] in label_encoders[col].classes_:\n            ex_df[f'{col}_encoded'] = label_encoders[col].transform([ex[col]])[0]\n        else:\n            ex_df[f'{col}_encoded'] = 0\n    \n    # Make ML prediction\n    ml_pred = model.predict(ex_df[feature_cols])[0]\n    \n    # Apply rules\n    hybrid_result = rules_engine.adjust_prediction(ml_pred, ex)\n    \n    print(f\"\\nExample {i}: {ex['year']} {ex['make']} {ex['model']}\")\n    print(f\"  Details: {ex['body']}, {ex['odometer']:,} miles, Condition {ex['condition']}/49\")\n    print(f\"  ML Prediction:     ${ml_pred:>10,.2f}\")\n    \n    if hybrid_result['adjustments']:\n        print(f\"  Rule Adjustments:\")\n        for rule_name, adjustment in hybrid_result['adjustments']:\n            print(f\"    - {rule_name}: ${adjustment:>+10,.2f}\")\n    else:\n        print(f\"  Rule Adjustments:  (none applied)\")\n    \n    print(f\"  Hybrid Price:      ${hybrid_result['adjusted_price']:>10,.2f}\")\n    print(f\"  Total Adjustment:  ${hybrid_result['total_adjustment']:>+10,.2f}\")\n    print(\"-\" * 90)\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"MODEL TRAINING COMPLETE!\")\nprint(\"=\"*70)\nprint(\"\\nFiles created:\")\nprint(\"  - vehicle_price_model.pkl\")\nprint(\"  - label_encoders.pkl\")\nprint(\"  - feature_columns.pkl\")\nprint(\"  - model_statistics.pkl (includes hybrid metrics)\")\nprint(\"  - make_model_mapping.pkl (for UI dropdowns)\")\nprint(\"  - feature_importance.png\")\nprint(\"  - hybrid_system_analysis.png\")\nprint(\"\\nAI Techniques Implemented:\")\nprint(\"  1. Statistical Learning (XGBoost) - Gradient Boosting\")\nprint(\"  2. Rule-Based Expert System - Knowledge-Based AI\")\nprint(\"\\nHybrid System Performance:\")\nprint(f\"  ML-only R²: {test_r2:.4f}\")\nprint(f\"  Hybrid R²:  {hybrid_r2:.4f} (improvement: {(hybrid_r2-test_r2)*100:+.2f}%)\")\nprint(f\"  Rules applied to {np.sum(rule_adjustments_list != 0) / len(rule_adjustments_list) * 100:.1f}% of predictions\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}